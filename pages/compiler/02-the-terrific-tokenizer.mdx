---
title: 'The Terrific Tokenizer'
blurb: 'How does a tokenizer convert a code string into a list of tokens?'
publishedAt: '2021-12-21'
editedAt: '2021-12-21'
---

import { Boilerplate } from '@/components/compiler/Boilerplate'
import { Tokenizer } from '@/components/compiler/Tokenizer'
import { PageList } from '@/components/compiler/PageList'
import { Editor } from '@/components/compiler/Editor'
import { Reveal } from '@/components/Reveal'
import Figure from '@/elements/Figure'
import Callout from '@/elements/Callout'
import ProblemStatement from '@/elements/ProblemStatement'

A few weekends ago, I spent some time rebuilding the Babel compiler from scratch to learn a bit more about how it works internally. You see, I already knew a _bit_ about compilers and talked about them briefly in my [debugger]() post, but there was a very large knowledge gap that was apparent to me. I knew how to manipulate an abstract syntax tree, but I didn't know _how to make one_.

Fundamentally, I see a compiler as a pipeline with four concrete steps:

// drawing here

Each step takes in the output of the previous step and transforms it to something else. The first step, the tokenization step, takes in your original source code as its input, while the last step, the generation step, spits out the modified code.

> It's more accurate to say that Babel is a _transpiler_ rather than a _compiler_, since the output is the same programming language as the input.

In this post, we'll focus on implementing that first step. Specifically, we'll build a tokenizer that can understand the following code snippet _and absolutely nothing more_:

```ts
function hello(message) {
  console.log(message)
}
```

And here's the final product in action:

<Figure size="lg">
  <Tokenizer />
</Figure>

## Tokens Are the Language's Words

But hold on a minute — what does a tokenizer actually _do_?

Essentially, a tokenizer breaks up your source code into small objects called **tokens** (hence the name). I like to think of a token as a "word" in the programming language, i.e. the smallest sequence of characters that still carry a meaning.

For example, if you tokenize the following JavaScript code:

```js
console.log(message)
```

You'll end up with the following six tokens:

```
[console] [.] [log] [(] [message] [)]
```

Just like how a word in english can be a noun, verb, adjective, etc., each token has a type that represents that token's meaning. For the above sequence of tokens, the types might be something like this:

```
[console]   [.]  [log]       [(]        [message]   [)]
 Identifier  Dot  Identifier  LeftParen  Identifier  RightParen
```

To reiterate, the tokenizer's job is to break up the source code (which it receives as a string) into a list of tokens like we just saw. Breaking up the code like this makes the other phases' lives much easier — instead of working with the source string directly, they work with the neat and tidy list of tokens.

## Implementation Plan

Here's the final tokenizer once again tokenizing our code snippet:

<Figure size="lg">
  <Tokenizer />
</Figure>

To implement it, we'll break it down into two parts:

1. Parsing single character tokens, and
2. Parsing identifiers and keywords.

## Single Character Tokens

Let's begin by trying to parse out the simplest tokens first - the ones that are only one character long. In the code snippet we're parsing, that would be the dot, curly brackets and left parentheses tokens:

<Figure size="lg">

![](single-tokens.png)

</Figure>

We'll start off by writing some boilerplate to iterate through every character of the string:

<Figure size="lg">
  <Boilerplate />
</Figure>

As we iterate through the code string, we check if the current character points to one of these single character tokens. If it does, we add the character to the final tokens list. We'll ignore everything else for now.

```ts highlight=5-8
function tokenize(input) {
  let current = 0
  let tokens = []

  while (current < code.length) {
    const char = code[current]
    if (isSingleCharacterToken(char)) {
      tokens.push(char)
    }
    current++
  }

  return tokens
}
```

One way to check if a character is a single character token is to keep a list of all single character tokens and checking if the character is in that list:

```ts
const singleCharacterTokens = new Set(['{', '}', '(', ')', '.'])

function isSingleCharacterToken(char) {
  return singleCharacterTokens.has(char)
}
```

Cool! If we run this piece of code now, we'll get something like this:

<Figure size="lg">Test</Figure>

## Identifiers and Keywords

With the single character tokens out of the way, the only thing left is the parsing the identifier and keyword tokens:

<Figure size="lg"></Figure>

<ProblemStatement>How do we parse identifier tokens?</ProblemStatement>

### What Makes an Identifier?

Before we can parse identifiers, we have to figure out what constitutes a valid identifier in the first place.

In JavaScript, an identifier is a sequence of characters that is used to refer to some piece of data. For example, in our input code snippet, the words "message" and "hello" are both identifiers because they refer to the function argument and the function definition respectively.

A _valid_ identifier in JavaScript is a sequence of alphanumeric characters, except the first character cannot be a number. This means the following strings are valid identifiers:

```
hello
_abc
abc123
```

But the following are not:

```
2cool
8ball
```

I wanted to keep things simple for this tokenizer so I limited it further to only alphabetic characters. This means my tokenizer will only recognize the word "hello" as a valid identifier out of the examples above.

### Implementation

To recap, an identifier (for our purposes) is any sequence of alphabetic characters. To parse it, I went with the following approach:

1. If the current character is alphabetic, start parsing an identifier;
2. Keep adding characters to the current identifier token until the current character is **not** alphabetic.

```ts highlight=3,7-14,18-19
const singleCharacterTokens = new Set(['(', ')', '{', '}', ';', '.'])

function tokenize(code: string): string[] {
  let start = 0
  let current = 0
  let tokens = []

  function finishIdentifier() {
    while (isAlpha(code[current])) {
      current++
    }
    const name = code.substring(start, current)
    start = current
    return name
  }

  while (current < code.length) {
    const char = code[current]
    if (isAlpha(char)) {
      tokens.push(finishIdentifier())
    } else if (singleCharacterTokens.has(char)) {
      tokens.push(char)
      start++
      current++
    }
  }

  return tokens
}
```

Visually, this approach looks like this:

<Figure size="lg"></Figure>

### Keywords

Some identifiers have a special meaning in JavaScript and therefore cannot be used to refer to a piece of data. This includes words like `function`, `while`, and `switch`. This group of identifiers are called **keywords** and typically have their own token type.

Therefore...

<ProblemStatement>
  How does the tokenizer differentiate between an identifier and a keyword?
</ProblemStatement>

One way is to do the same thing as the single character tokens:

1. Keep a set of known keywords;
2. When we're parsing an identifier, check if the parsed name is in this set;
3. If it is, change the token's type to the keyword's type.

<Figure size="lg">keywords</Figure>

## Whitespace

Great! So far, our identifier can identify single character, identifier, and keyword tokens. The last thing we need to take care of is _whitespace_. We don't really want to do anything with whitespace, so the only change here is to skip the current character if it's a whitespace character.

<Figure size="lg">Ignore whitespace</Figure>

## Summary

And there we have our tokenizer! It can't do too much at the moment, but it's able to tokenize the code snippet that we started with:

```js
function hello(message) {
  console.log(message)
}
```

I purposely omitted code snippets because I wanted to solidify the _concepts_ behind a tokenizer rather than tying it down to any direct implementation. After all, there's a bunch of different ways of implementing the same thing! But if you'd like to see an implementation, check out [my implementation of this tokenizer (written in TypeScript)](https://github.com/narendrasss/compiler/blob/main/src/tokenizer.ts).

To finish off, I have a few exercises for you to try out if you'd like to learn more:

1. **Implement this tokenizer in your language of choice**; use the visualization from the introduction as a reference.
2. Once it's done, **extend the tokenizer to support the following syntax**: `const hello = 'world'`.

I'd love to see what you come up with, and thanks for reading!
