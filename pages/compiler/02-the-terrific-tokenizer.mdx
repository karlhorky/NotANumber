---
title: 'The Terrific Tokenizer'
blurb: 'How does a tokenizer convert a code string into a list of tokens?'
publishedAt: '2021-12-21'
editedAt: '2021-12-21'
---

import { Boilerplate } from '@/components/compiler/Boilerplate'
import { Tokenizer } from '@/components/compiler/Tokenizer'
import { PageList } from '@/components/compiler/PageList'
import { Editor } from '@/components/compiler/Editor'
import { Reveal } from '@/components/Reveal'
import Figure from '@/elements/Figure'
import Callout from '@/elements/Callout'
import ProblemStatement from '@/elements/ProblemStatement'

A few weekends ago, I spent some time rebuilding the Babel compiler from scratch to learn a bit more about how it works internally. You see, I already knew a _bit_ about compilers and talked about them briefly in my [debugger]() post, but there was a very large knowledge gap that was apparent to me. I knew how to manipulate an abstract syntax tree, but I didn't know _how to make one_.

Fundamentally, I see a compiler as a pipeline with four concrete steps:

// drawing here

Each step takes in the output of the previous step and transforms it to something else. The first step, the tokenization step, takes in your original source code as its input, while the last step, the generation step, spits out the modified code.

> It's more accurate to say that Babel is a _transpiler_ rather than a _compiler_, since the output is the same programming language as the input.

In this post, we'll focus on implementing that first step. Specifically, we'll build a tokenizer that can understand the following code snippet _and absolutely nothing more_:

```ts
function hello(message) {
  console.log(message)
}
```

And here's the final product in action:

<Figure size="lg">
  <Tokenizer />
</Figure>

## Tokens are the language's words

But hold on a minute — what does a tokenizer actually _do_?

Essentially, a tokenizer breaks up your source code into small objects called **tokens** (hence the name). I like to think of a token as a "word" in the programming language — the smallest sequence of characters that still carry a meaning.

For example, if you tokenize the following JavaScript code:

```js
console.log(message)
```

You'll end up with the following six tokens:

```
[console] [.] [log] [(] [message] [)]
```

Each token has a type that represents that token's meaning. For the above sequence of tokens, the types might be something like this:

```
[console]   [.]  [log]       [(]        [message]   [)]
 Identifier  Dot  Identifier  LeftParen  Identifier  RightParen
```

To reiterate, the tokenizer's job is to break up the source code (which it receives as a string) into a list of tokens like we just saw. Breaking up the code like this makes the other phases' lives much easier — instead of working with the source string directly, they work with the neat and tidy list of tokens.

## Implementation Plan

Here's the final tokenizer once again tokenizing our code snippet:

<Figure size="lg">
  <Tokenizer />
</Figure>

To implement it, we'll break it down into two parts:

1. Parsing single character tokens, and
2. Parsing identifiers and keywords.

## Single Character Tokens

Let's begin by trying to parse out the simplest tokens first - the ones that are only one character long. In the code snippet we're parsing, that would be the dot, curly brackets and left parentheses tokens:

<Figure size="lg">

![](single-tokens.png)

</Figure>

We'll start off by writing some boilerplate to iterate through every character of the string:

<Figure size="lg">
  <Boilerplate />
</Figure>

As we iterate through the code string, we check if the current character points to one of these single character tokens. If it does, we add the character to the final tokens list. We'll ignore everything else for now.

```ts highlight=5-8
function tokenize(input) {
  let current = 0
  let tokens = []

  while (current < code.length) {
    const char = code[current]
    if (isSingleCharacterToken(char)) {
      tokens.push(char)
    }
    current++
  }

  return tokens
}
```

One way to check if a character is a single character token is to keep a list of all single character tokens and checking if the character is in that list:

```ts
const singleCharacterTokens = new Set(['{', '}', '(', ')', '.'])

function isSingleCharacterToken(char) {
  return singleCharacterTokens.has(char)
}
```

Cool! If we run this piece of code now, we'll get something like this:

<Figure size="lg">Test</Figure>

## Identifiers and Keywords

With the single character tokens out of the way, the only thing left is the parsing the identifier and keyword tokens:

<Figure size="lg"></Figure>

<ProblemStatement>How do we parse identifier tokens?</ProblemStatement>

### What Makes an Identifier?

Before we can parse identifiers, we have to figure out what constitutes a valid identifier in the first place.

In JavaScript, an identifier is a sequence of characters that is used to refer to some piece of data. For example, in our input code snippet, the words "message" and "hello" are both identifiers because they refer to the function argument and the function definition respectively.

A _valid_ identifier in JavaScript is a sequence of alphanumeric characters, except the first character cannot be a number. This means the following strings are valid identifiers:

```
hello
_abc
abc123
```

But the following are not:

```
2cool
8ball
```

I wanted to keep things simple for this tokenizer so I limited it further to only alphabetic characters. This means my tokenizer will only recognize the word "hello" as a valid identifier out of the examples above.

### Implementation

To recap, an identifier (for our purposes) is any sequence of alphabetic characters. To parse it, I went with the following approach:

1. If the current character is alphabetic, start parsing an identifier;
2. Keep adding characters to the current identifier token until the current character is **not** alphabetic.

```ts highlight=3,7-14,18-19
const singleCharacterTokens = new Set(['(', ')', '{', '}', ';', '.'])

function tokenize(code: string): string[] {
  let start = 0
  let current = 0
  let tokens = []

  function finishIdentifier() {
    while (isAlpha(code[current])) {
      current++
    }
    const name = code.substring(start, current)
    start = current
    return name
  }

  while (current < code.length) {
    const char = code[current]
    if (isAlpha(char)) {
      tokens.push(finishIdentifier())
    } else if (singleCharacterTokens.has(char)) {
      tokens.push(char)
      start++
      current++
    }
  }

  return tokens
}
```

Visually, this approach looks like this:

<Figure size="lg"></Figure>

### Keywords

Some identifiers have a special meaning in JavaScript and therefore cannot be used to refer to a piece of data. This includes words like "function", "while", and "switch". This group of identifiers are called **keywords** and typically have their own token type.

<ProblemStatement>
  How does the tokenizer differentiate between an identifier and a keyword?
</ProblemStatement>

### Whitespace

Finally, the last thing we need to take care of is _whitespace_. We don't really want to do anything with whitespace, so we will simply skip the current character if it's whitespace:

```ts
const singleCharacterTokens = new Set(['(', ')', '{', '}', ';', '.'])

function isAlpha(char: string) {
  return /[a-zA-Z]/.test(char)
}

function isWhitespace(char: string) {
  return /\s/.test(char)
}

function tokenize(code: string): string[] {
  let start = 0
  let current = 0
  let tokens = []

  function finishIdentifier() {
    while (isAlpha(code[current])) {
      current++
    }
    const name = code.substring(start, current)
    start = current
    return name
  }

  while (current < code.length) {
    const char = code[current]
    if (isAlpha(char)) {
      tokens.push(finishIdentifier())
    } else if (singleCharacterTokens.has(char)) {
      tokens.push(char)
      start++
      current++
    } else if (isWhitespace(char)) {
      start++
      current++
    }
  }

  return tokens
}
```

And that's it! We're now able to parse enough tokens to recognize all of the tokens of our input:

```js
function hello(message) {
  console.log(message)
}
```

## Refactoring

Our code has one issue though: we're only returning the tokens as literal strings! This isn't much more useful for the rest of the compiler pipeline than the initial raw code is. It would be a lot nicer if we return some information about the token's _type_ as well. This would also let us differentiate between _identifiers_ and _keywords_, which we haven't done so far.

Let's start by creating a list of all the token types:

```ts
enum TokenType {
  Keyword = 'Keyword',
  Identifier = 'Identifier',
  LeftParen = 'LeftParen',
  RightParen = 'RightParen',
  LeftCurly = 'LeftCurly',
  RightCurly = 'RightCurly',
  Dot = 'Dot',
  Semicolon = 'Semicolon',
}
```

_I've purposely assigned each enum value to a string here so that it's easier to debug; if we didn't do this, logging the token type would only show a number like 0 or 1._

The token object that we're going to be using consists of a `type` property and an optional `name` property for keywords and identifiers:

```ts
type Token = {
  type: TokenType
  name?: string
}
```

To make it easier to create these objects, let's make builder functions for each token type:

```ts
const token = {
  keyword(name: string) {
    return {
      type: TokenType.Keyword,
      name,
    }
  },
  identifier(name: string) {
    return {
      type: TokenType.Identifier,
      name,
    }
  },
  leftParen() {
    return { type: TokenType.LeftParen }
  },
  rightParen() {
    return { type: TokenType.RightParen }
  },
  leftCurly() {
    return { type: TokenType.LeftCurly }
  },
  rightCurly() {
    return { type: TokenType.RightCurly }
  },
  dot() {
    return { type: TokenType.Dot }
  },
  semicolon() {
    return { type: TokenType.Semicolon }
  },
}
```

This lets us construct a token by simply calling `token.<type>()`, e.g. `token.keyword('function')` to create a token that corresponds to the `function` keyword.

Great! All that's left is to update our `tokenize` function to use these builder functions instead of returning the string literal. First, let's update the `finishIdentifier()` function to create an identifier or a keyword:

```ts
const keywords = new Set(['function'])

function tokenize(code: string): string[] {
  let start = 0
  let current = 0
  let tokens = []

  function finishIdentifier() {
    while (isAlpha(code[current])) {
      current++
    }
    const name = code.substring(start, current)
    start = current

    if (keywords.has(name)) {
      return token.keyword(name)
    }

    return token.identifier(name)
  }

  while (current < code.length) {
    const char = code[current]
    if (isAlpha(char)) {
      tokens.push(finishIdentifier())
    } else if (singleCharacterTokens.has(char)) {
      tokens.push(char)
      start++
      current++
    } else if (isWhitespace(char)) {
      start++
      current++
    }
  }

  return tokens
}
```

Notice that I also added a set to contain the list of known keywords. For now, since we're only parsing the hello function, the only known keyword is "function".

For the single character tokens, we need a way to map each character string to the appropriate builder function. For example, if we see the string `";"` then we need to call the `token.semicolon()` function.

To do this, I created a `Map` that defines this relationship:

```ts
type SingleCharacterToken = '(' | ')' | '{' | '}' | '.' | ';'

const knownSingleCharacters = new Map<SingleCharacterToken, () => Token>([
  ['(', token.leftParen],
  [')', token.rightParen],
  ['{', token.leftCurly],
  ['}', token.rightCurly],
  ['.', token.dot],
  [';', token.semicolon],
])
```

And added a type guard to determine if the current character is a single character token:

```ts
function isSingleCharacter(char: string): char is SingleCharacterToken {
  return knownSingleCharacters.has(char as SingleCharacterToken)
}
```

Then I added another helper that returns the appropriate builder function for a single character token:

```ts
function getCharToken(char: SingleCharacterToken) {
  const builder = knownSingleCharacters.get(char)
  // we need the exclamation mark here because TS would say
  // that this is undefined otherwise.
  return builder!()
}
```

Now we can update the `tokenize` function to call these helpers:

```ts
function tokenize(input: string): Token[] {
  let start = 0
  let current = 0
  const tokens = []

  function finishIdentifier() {
    while (isAlpha(input[current])) {
      current++
    }
    const name = input.substring(start, current)
    start = current

    if (keywords.has(name)) {
      return token.keyword(name)
    }

    return token.identifier(name)
  }

  while (current < input.length) {
    const currentChar = input[current]

    if (isWhitespace(currentChar)) {
      start++
      current++
      continue
    }

    if (isAlpha(currentChar)) {
      tokens.push(finishIdentifier())
    } else if (isSingleCharacter(currentChar)) {
      tokens.push(getCharToken(currentChar))
      start++
      current++
    }
  }

  return tokens
}
```

And as a finishing touch, let's throw an error if we find a character we can't parse just yet:

```ts
function tokenize(input: string): Token[] {
  let start = 0
  let current = 0
  const tokens = []

  function finishIdentifier() {
    while (isAlpha(input[current])) {
      current++
    }
    const name = input.substring(start, current)
    start = current

    if (keywords.has(name)) {
      return token.keyword(name)
    }

    return token.identifier(name)
  }

  while (current < input.length) {
    const currentChar = input[current]

    if (isWhitespace(currentChar)) {
      start++
      current++
      continue
    }

    if (isAlpha(currentChar)) {
      tokens.push(finishIdentifier())
    } else if (isSingleCharacter(currentChar)) {
      tokens.push(getCharToken(currentChar))
      start++
      current++
    } else {
      throw new Error(`Unknown character: ${currentChar}`)
    }
  }

  return tokens
}
```

Now if we run this function, we get exactly the output we would expect!

```
exec tokenizer
```

## Summary

And there we have our tokenizer! It can't do too much at the moment, but it's able to tokenize the input that we want it to - this simple hello function:

```js
function hello(message) {
  console.log(message)
}
```

It might also seem a tad over-engineered, but I think it's general enough that we can extend it later on when we implement more features of the JS language.

In the next post, we'll take the tokens generated in this phase and try to parse them into a syntax tree. Until then!
